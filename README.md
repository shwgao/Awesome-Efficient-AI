# Awesome Efficient AI
I am sharing awesome papers related to the HPC, Efficient AI and AI 4 Science etc..

Topics:
- [AI Compilers](#ai-compilers)
- [CUDA](#cuda)
- [NNFusion](#nnfusion)
- [Efficient AI Algorithms](#efficient-ai-algorithms)

## AI Compilers

| Paper | Conference & Year |
|-------|-------------------|
|[TVM: An Automated End-to-End Optimizing Compiler for Deep Learning](./docs/AI%20Compilers/TVM/TVM.md)| OSDI, 2018 |
|[Pytorch 2.0 compiler: torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html#id3)|          |
|[Welder: Scheduling Deep Learning Memory Access via Tile-graph](./docs/AI%20Compilers/Welder/welder.md)|OSDI, 2023 |

## CUDA
| Paper | Conference & Year |
|-------|-------------------|
|[CUDA basic knowledge](./docs/CUDA/CUDA_basic_knowledge/%20CUDA_basics.md)||
|[Mirage: A Multi-Level Superoptimizer for Tensor Programs <br>(Generating Fast GPU Kernels without Programming in CUDA/Triton)](./docs/CUDA/Mirage/Mirage.md)|Arxiv, 2024|

## NNFusion
| Paper | Conference & Year |
|-------|-------------------|

## Efficient AI Algorithms
| Paper | Conference & Year |
|-------|-------------------|
|[SELF-ATTENTION DOES NOT NEED $O(n^{2})$ MEMORY](./docs/Efficient%20AI%20Algorithems/efficient-attention-memory.md)|arXiv, 2021|
|[G10: Enabling An Efficient Unified GPU Memory and Storage Architecture with Smart Tensor Migrations](./docs/Efficient%20AI%20Algorithems/G10/G10.md)|MICRO, 2023|

## Memory Management
| Paper | Conference & Year |
|-------|-------------------|
|[Dapple:A pipelined data parallel approach for training large models.](.)|PPoPP, 2021|
|[Zero-offload: Democratizing billion-scale modeltraining](.)|USENIX, 2021|
|[Memory-efficient pipeline-parallel dnn training](.)|PMLR, 2021|
|[Zero:Memory optimizations toward training trillion parameter models.](.)|SC20, 2020|
|[Gpipe: Efficient training of giant neural networks using pipeline parallelism](.)|NeurIPS, 2019|